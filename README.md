## Detecting Contradiction and Entailment In Multilingual Text Using Self-Attention Transformer Models - BERT, DistilBERT and RoBERTa
Training data is pairs of two sentences (consisting of a premise and a hypothesis) classified into three categories Entailment, Neutral or Contradiction. Training data contains premise hypothesis pairs in fifteen different languages, including: Arabic, Bulgarian, Chinese, German, Greek, English, Spanish, French, Hindi, Russian, Swahili, Thai, Turkish, Urdu, and Vietnamese is avaiable at: https://www.kaggle.com/c/contradictory-my-dear-watson/data 

## BERT, DistilBERT and RoBERTa model accuracies
![model_accuracy1](https://user-images.githubusercontent.com/78239454/129111082-5653a210-10ea-466c-b592-72adbdcad1a1.png)

## XLM-RoBERTa-Large Confusion Matrix 


## Language distribution in training data 
![language_train_pie (1)](https://user-images.githubusercontent.com/78239454/129109653-7c6b5f5a-ef0c-4d9e-92af-beb32c7982c5.png)

## Label distribution in training data 
![_plotly_label_pie (1)](https://user-images.githubusercontent.com/78239454/129109690-f8cd4b05-dd73-4c16-8128-bd3f7bb8fd4d.png)

## Label distribution by language in training data 
![_language_label_count (1)](https://user-images.githubusercontent.com/78239454/129109692-b05b7c29-5834-49ad-adb5-04cd0d3ef77e.png)
